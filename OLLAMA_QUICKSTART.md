# üöÄ Guide de d√©marrage rapide - LLM Local avec Ollama

Ce guide vous aide √† configurer un LLM local (Qwen, Llama, Mistral) pour QUIZO en 10 minutes.

## ‚úÖ √âtape 1 : Installer Ollama (5 minutes)

### Windows

```powershell
# Option 1 : T√©l√©chargement direct
# Allez sur https://ollama.com/download et t√©l√©chargez l'installateur Windows

# Option 2 : Avec winget
winget install Ollama.Ollama
```

Apr√®s installation, **red√©marrez votre ordinateur** pour que Ollama d√©marre automatiquement.

### V√©rifier l'installation

```powershell
# Ouvrir PowerShell et taper :
ollama --version

# Devrait afficher : ollama version is 0.x.x
```

---

## ‚úÖ √âtape 2 : T√©l√©charger un mod√®le (5 minutes)

### Mod√®le recommand√© pour d√©marrer : Qwen 2.5 (7B)

```powershell
# T√©l√©charger Qwen 2.5 - Excellent pour le fran√ßais
ollama pull qwen2.5:7b

# ‚è≥ Cela prendra 5-10 minutes (t√©l√©charge ~4.7 GB)
```

### Autres options selon votre mat√©riel

```powershell
# PC modeste (8GB RAM, pas de GPU) :
ollama pull phi3:mini          # L√©ger (2.3 GB)

# PC puissant (16GB+ RAM, bon GPU) :
ollama pull qwen2.5:14b        # Meilleure qualit√© (8.5 GB)

# Alternative fran√ßaise excellente :
ollama pull mistral:7b         # Rapide et performant (4.1 GB)
```

### V√©rifier les mod√®les t√©l√©charg√©s

```powershell
ollama list

# Affiche les mod√®les disponibles
```

---

## ‚úÖ √âtape 3 : Tester Ollama (2 minutes)

### Test interactif

```powershell
ollama run qwen2.5:7b

# Dans le chat qui s'ouvre, demander :
# "G√©n√®re une question QCM en fran√ßais sur Firebase avec 4 options"

# Pour quitter : /bye
```

### Test de l'API

```powershell
# V√©rifier que l'API fonctionne
curl http://localhost:11434/api/tags

# Devrait retourner la liste des mod√®les en JSON
```

---

## ‚úÖ √âtape 4 : Configurer QUIZO (1 minute)

### Cr√©er/modifier le fichier `.env`

Dans le dossier `python_api/`, cr√©ez ou √©ditez `.env` :

```bash
# Configuration Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b

# Vos autres cl√©s API (optionnelles)
GEMINI_API_KEY=votre_cl√©_si_vous_en_avez_une
CHATGPT_API_KEY=votre_cl√©_si_vous_en_avez_une
```

### Frontend `.env`

Dans le dossier racine, cr√©ez ou √©ditez `.env` :

```bash
VITE_BACKEND_URL=http://localhost:5001/api
VITE_LOCAL_LLM_URL=http://localhost:11434
```

---

## ‚úÖ √âtape 5 : Tester avec QUIZO

### D√©marrer le backend Flask

```powershell
cd python_api
python app.py

# Devrait afficher :
# INFO - Ollama configur√© sur: http://localhost:11434 avec mod√®le par d√©faut: qwen2.5:7b
# INFO - D√©marrage du serveur Flask sur le port 5001
```

### Tester le service Ollama Python

```powershell
# Dans le dossier python_api
python ollama_service.py

# Devrait afficher :
# ‚úÖ Ollama est disponible!
# üì¶ Mod√®les disponibles: qwen2.5:7b
# ‚úÖ 2 questions g√©n√©r√©es avec succ√®s!
```

### Utiliser dans l'interface QUIZO

1. Ouvrir QUIZO dans le navigateur
2. Aller sur la page "Cr√©er un Quiz"
3. Uploader un fichier PDF/DOCX
4. Dans "Type de mod√®le", s√©lectionner **"LLM Local"**
5. S√©lectionner votre mod√®le (ex: qwen2.5:7b)
6. Cliquer sur "G√©n√©rer le Quiz"

---

## üîß D√©pannage

### Probl√®me : "Ollama n'est pas disponible"

```powershell
# Solution 1 : V√©rifier qu'Ollama tourne
Get-Process ollama

# Si rien n'appara√Æt, d√©marrer Ollama manuellement :
ollama serve

# Ensuite dans un autre terminal :
ollama list
```

### Probl√®me : "Le mod√®le qwen2.5:7b n'est pas disponible"

```powershell
# V√©rifier les mod√®les t√©l√©charg√©s
ollama list

# Si vide, t√©l√©charger :
ollama pull qwen2.5:7b
```

### Probl√®me : G√©n√©ration trop lente

```powershell
# Solution 1 : Utiliser un mod√®le plus petit
ollama pull phi3:mini
ollama run phi3:mini

# Solution 2 : V√©rifier l'utilisation du GPU (NVIDIA)
nvidia-smi   # Doit montrer "ollama" dans les processus

# Solution 3 : R√©duire le texte source
# Dans QUIZO, uploader des fichiers plus petits (<5 pages)
```

### Probl√®me : Erreur de m√©moire

```powershell
# Solution : Utiliser un mod√®le plus l√©ger
ollama pull phi3:mini   # N√©cessite seulement 4GB RAM

# Ou fermer les autres applications
# Chrome, VSCode, etc.
```

---

## üìä Comparaison de performance

Test√© sur un PC avec **16GB RAM, RTX 3060 (12GB VRAM)** :

| Mod√®le | Temps/question | Qualit√© | RAM utilis√©e |
|--------|---------------|---------|--------------|
| qwen2.5:7b | ~8s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 6 GB |
| qwen2.5:14b | ~15s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 10 GB |
| llama3.1:8b | ~10s | ‚≠ê‚≠ê‚≠ê‚≠ê | 7 GB |
| mistral:7b | ~7s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 6 GB |
| phi3:mini | ~4s | ‚≠ê‚≠ê‚≠ê | 3 GB |
| **Gemini API** | ~5s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 0 GB |

---

## üéØ Utilisation avanc√©e

### T√©l√©charger plusieurs mod√®les

```powershell
# Avoir le choix selon vos besoins
ollama pull qwen2.5:7b      # Qualit√©
ollama pull phi3:mini       # Vitesse
ollama pull mistral:7b      # √âquilibre
```

### Changer de mod√®le dans QUIZO

1. Backend : modifier `OLLAMA_MODEL` dans `.env`
2. Frontend : s√©lectionner dans le dropdown "Mod√®le Local"
3. Les mod√®les sont auto-d√©tect√©s depuis Ollama

### Optimiser pour production

```bash
# .env pour production
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b
LOG_LEVEL=WARNING   # Moins de logs
```

---

## üéâ F√©licitations !

Vous avez maintenant un LLM **100% gratuit et priv√©** pour QUIZO !

### Avantages :
- ‚úÖ **Gratuit** : Pas de co√ªts d'API
- ‚úÖ **Priv√©** : Vos donn√©es restent locales
- ‚úÖ **Illimit√©** : G√©n√©rez autant de quiz que vous voulez
- ‚úÖ **Hors ligne** : Fonctionne sans Internet (apr√®s t√©l√©chargement)

### Prochaines √©tapes :
- Tester diff√©rents mod√®les pour trouver le meilleur pour vous
- Cr√©er des quiz sur vos cours
- Partager avec vos √©tudiants
- Contribuer au projet QUIZO sur GitHub

---

## üìö Ressources

- **Ollama** : https://ollama.com
- **Qwen Models** : https://huggingface.co/Qwen
- **Mistral** : https://mistral.ai
- **Llama** : https://ai.meta.com/llama

## üí¨ Support

Si vous rencontrez des probl√®mes :
1. V√©rifier la section [D√©pannage](#-d√©pannage) ci-dessus
2. Consulter les logs : `python_api/app.py` affiche des messages d√©taill√©s
3. Ouvrir une issue sur GitHub

---

**Bon quiz ! üéì**
