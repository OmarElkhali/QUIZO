# Guide d'impl√©mentation d'un LLM Local pour QUIZO

Ce guide vous montre comment impl√©menter un LLM open-source local (comme Qwen) pour remplacer ou compl√©ter Gemini/ChatGPT dans votre application QUIZO.

## üìã Table des mati√®res

1. [Pourquoi un LLM local ?](#pourquoi-un-llm-local)
2. [Option 1 : Ollama (Recommand√©)](#option-1-ollama-recommand√©)
3. [Option 2 : LM Studio](#option-2-lm-studio)
4. [Option 3 : Serveur Python avec Transformers](#option-3-serveur-python-avec-transformers)
5. [Int√©gration dans QUIZO](#int√©gration-dans-quizo)

---

## Pourquoi un LLM local ?

### ‚úÖ Avantages
- **Gratuit** : Pas de co√ªts d'API
- **Priv√©** : Vos donn√©es restent locales
- **Contr√¥le total** : Personnalisation compl√®te
- **Pas de limites de taux** : Utilisez autant que vous voulez
- **Open-source** : Mod√®les comme Qwen, Llama, Mistral

### ‚ùå Inconv√©nients
- N√©cessite plus de ressources (RAM, GPU recommand√©)
- Plus lent que les API cloud (selon votre mat√©riel)
- Configuration initiale plus complexe

### üéØ Mod√®les recommand√©s pour QUIZO
1. **Qwen2.5-7B** - Excellent pour le fran√ßais, questions QCM
2. **Llama-3.1-8B** - Tr√®s performant, multilingue
3. **Mistral-7B** - Rapide, bon en fran√ßais
4. **Phi-3** - L√©ger, parfait pour PC modestes

---

## Option 1 : Ollama (Recommand√©)

### Installation

#### Windows
```powershell
# T√©l√©charger depuis https://ollama.com/download
# Ou avec winget
winget install Ollama.Ollama
```

#### D√©marrer Ollama
```powershell
# Ollama d√©marre automatiquement apr√®s installation
# V√©rifier que √ßa tourne
ollama list
```

### T√©l√©charger Qwen2.5
```powershell
# Mod√®le 7B (recommand√© - ~4.7 GB)
ollama pull qwen2.5:7b

# Ou version 14B si vous avez une bonne carte graphique (~8.5 GB)
ollama pull qwen2.5:14b

# Ou version fran√ßaise optimis√©e
ollama pull qwen2.5:7b-instruct
```

### Tester Qwen localement
```powershell
# Test interactif
ollama run qwen2.5:7b

# Demander une question QCM
# "G√©n√®re une question QCM en fran√ßais sur Firebase avec 4 options"
```

### Serveur Ollama API
Ollama expose automatiquement une API REST sur `http://localhost:11434`

```powershell
# Tester l'API
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b",
  "prompt": "G√©n√®re une question QCM en fran√ßais",
  "stream": false
}'
```

---

## Option 2 : LM Studio

### Installation
1. T√©l√©charger depuis https://lmstudio.ai/
2. Installer l'application
3. Dans l'interface, chercher "Qwen 2.5" ou "Llama 3.1"
4. T√©l√©charger le mod√®le (format GGUF)
5. D√©marrer le serveur local (onglet "Local Server")

### Configuration
- Port par d√©faut : `http://localhost:1234`
- Compatible avec OpenAI API
- Interface graphique pour tester les prompts

---

## Option 3 : Serveur Python avec Transformers

### Installation des d√©pendances
```bash
pip install transformers torch accelerate bitsandbytes flask
```

### Code du serveur (voir `local_llm_server.py` cr√©√©)

Avantages :
- Contr√¥le total sur le mod√®le
- Peut utiliser quantization (4-bit, 8-bit) pour √©conomiser la RAM
- Personnalisation compl√®te

---

## Int√©gration dans QUIZO

### 1. Cr√©er un nouveau service backend

Voir fichiers cr√©√©s :
- `python_api/local_llm_server.py` - Serveur LLM local
- `python_api/ollama_service.py` - Int√©gration Ollama
- `src/services/localLlmService.ts` - Client frontend

### 2. Modifier le backend Flask existant

Ajout d'une option `local` dans `modelType` :
```python
model_type = data.get('modelType', 'gemini')
if model_type == 'local':
    # Utiliser Ollama ou serveur local
    content = generate_with_local_llm(prompt)
```

### 3. Mettre √† jour le frontend

Dans `CreateQuiz.tsx`, ajouter l'option "LLM Local" :
```typescript
<option value="local">Qwen Local (Gratuit)</option>
```

---

## Configuration recommand√©e

### Pour PC avec GPU NVIDIA (16GB+ VRAM)
```bash
ollama pull qwen2.5:14b
# Ou Llama 3.1 70B en quantization
```

### Pour PC sans GPU ou GPU modeste
```bash
ollama pull qwen2.5:7b
# Ou Phi-3 Mini
ollama pull phi3:mini
```

### Pour serveur/production
- Utiliser LM Studio ou serveur Python avec quantization 4-bit
- Load balancing entre plusieurs mod√®les
- Cache des r√©ponses courantes

---

## Comparaison de performance

| Mod√®le | Taille | RAM requise | Qualit√© (FR) | Vitesse |
|--------|--------|-------------|--------------|---------|
| Qwen2.5-7B | 4.7 GB | 8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Rapide |
| Llama-3.1-8B | 5 GB | 10 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | Rapide |
| Mistral-7B | 4.1 GB | 8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Tr√®s rapide |
| Phi-3 Mini | 2.3 GB | 4 GB | ‚≠ê‚≠ê‚≠ê | Ultra rapide |
| Gemini API | - | - | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Variable |
| ChatGPT API | - | - | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Variable |

---

## Prochaines √©tapes

1. **Installer Ollama** (5 minutes)
2. **T√©l√©charger Qwen2.5** (`ollama pull qwen2.5:7b`)
3. **Tester l'API** (voir exemples ci-dessus)
4. **Int√©grer dans QUIZO** (utiliser les fichiers cr√©√©s)
5. **Optimiser les prompts** pour la g√©n√©ration de QCM

---

## Ressources

- Ollama : https://ollama.com/
- Qwen Models : https://huggingface.co/Qwen
- LM Studio : https://lmstudio.ai/
- Transformers : https://huggingface.co/docs/transformers

---

## Support et Debug

### Probl√®me : Ollama ne d√©marre pas
```powershell
# Red√©marrer le service
Stop-Process -Name "ollama" -Force
ollama serve
```

### Probl√®me : M√©moire insuffisante
- Utiliser un mod√®le plus petit (Phi-3 Mini)
- Activer la quantization 4-bit
- Fermer les autres applications

### Probl√®me : G√©n√©ration trop lente
- V√©rifier que GPU est utilis√© (`nvidia-smi`)
- R√©duire la longueur du prompt
- Utiliser un mod√®le plus petit

