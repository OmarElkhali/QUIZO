# ‚ö° Commandes rapides - LLM Local QUIZO

## üöÄ Installation (premi√®re fois uniquement)

### Installer Ollama
```powershell
# Option 1 : Automatique (recommand√©)
.\setup-ollama.ps1

# Option 2 : Avec winget
winget install Ollama.Ollama

# Option 3 : Manuel
# Aller sur https://ollama.com/download
```

### T√©l√©charger le mod√®le recommand√©
```powershell
ollama pull qwen2.5:7b
```

---

## üéØ Utilisation quotidienne

### D√©marrer QUIZO avec LLM local

```powershell
# Terminal 1 : Backend
cd python_api
python app.py

# Terminal 2 : Frontend
npm run dev

# Ouvrir : http://localhost:5173
```

---

## üì¶ Gestion des mod√®les

### Lister les mod√®les install√©s
```powershell
ollama list
```

### T√©l√©charger un mod√®le
```powershell
# Pour PC standard (8-16GB RAM)
ollama pull qwen2.5:7b        # 4.7 GB - Recommand√©

# Pour PC modeste (4-8GB RAM)
ollama pull phi3:mini         # 2.3 GB - L√©ger

# Pour PC puissant (32GB+ RAM, bon GPU)
ollama pull qwen2.5:14b       # 8.5 GB - Qualit√© max

# Alternatives
ollama pull mistral:7b        # 4.1 GB - Rapide, excellent FR
ollama pull llama3.1:8b       # 5 GB - Multilingue
```

### Supprimer un mod√®le
```powershell
ollama rm qwen2.5:7b
```

### Tester un mod√®le interactivement
```powershell
ollama run qwen2.5:7b

# Dans le chat :
# "G√©n√®re une question QCM en fran√ßais sur Firebase"

# Pour quitter : /bye
```

---

## üß™ Tests

### Test Python du service Ollama
```powershell
cd python_api
python ollama_service.py
```

**R√©sultat attendu :**
```
‚úÖ Ollama est disponible!
üì¶ Mod√®les disponibles: qwen2.5:7b
‚úÖ 2 questions g√©n√©r√©es avec succ√®s!
```

### Test de l'API Flask
```powershell
# Terminal 1 : D√©marrer le backend
cd python_api
python app.py

# Terminal 2 : Tester l'endpoint
curl -X POST http://localhost:5001/api/generate `
  -H "Content-Type: application/json" `
  -d '{
    "text": "Firebase est une plateforme...",
    "numQuestions": 2,
    "difficulty": "easy",
    "modelType": "local",
    "localModel": "qwen2.5:7b"
  }'
```

### V√©rifier l'√©tat du service
```powershell
curl http://localhost:5001/api/health
```

**R√©sultat attendu :**
```json
{
  "status": "ok",
  "version": "1.0.0",
  "services": {
    "gemini": true,
    "chatgpt": false,
    "ollama": true,
    "ollama_models": ["qwen2.5:7b"]
  }
}
```

---

## üîß D√©pannage

### Ollama ne d√©marre pas
```powershell
# V√©rifier si Ollama tourne
Get-Process ollama

# Si non, d√©marrer manuellement
ollama serve

# Dans un autre terminal, v√©rifier
ollama list
```

### V√©rifier l'API Ollama
```powershell
curl http://localhost:11434/api/tags
```

### R√©installer Ollama
```powershell
# D√©sinstaller
winget uninstall Ollama.Ollama

# Supprimer les donn√©es (optionnel)
Remove-Item -Recurse -Force "$env:USERPROFILE\.ollama"

# R√©installer
winget install Ollama.Ollama

# Red√©marrer l'ordinateur
Restart-Computer
```

### Lib√©rer de la m√©moire
```powershell
# Arr√™ter Ollama
Stop-Process -Name "ollama" -Force

# Red√©marrer
ollama serve

# Ou red√©marrer Windows
Restart-Computer
```

---

## üìä Informations syst√®me

### V√©rifier la RAM disponible
```powershell
Get-CimInstance Win32_OperatingSystem | 
  Select-Object FreePhysicalMemory, TotalVisibleMemorySize |
  Format-List
```

### V√©rifier le GPU (NVIDIA)
```powershell
nvidia-smi
```

### V√©rifier l'espace disque
```powershell
Get-PSDrive C | Select-Object Used, Free
```

---

## ‚öôÔ∏è Configuration

### Cr√©er/Modifier .env backend
```powershell
# Cr√©er depuis l'exemple
cd python_api
Copy-Item .env.example .env

# √âditer
notepad .env
```

**Contenu minimal :**
```bash
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b
LOG_LEVEL=INFO
CORS_ORIGINS=http://localhost:5173
```

### Cr√©er/Modifier .env frontend
```powershell
# Cr√©er
cd ..
Copy-Item .env.example .env

# √âditer
notepad .env
```

**Ajouter :**
```bash
VITE_BACKEND_URL=http://localhost:5001/api
VITE_LOCAL_LLM_URL=http://localhost:11434
```

---

## üéì Sc√©narios d'utilisation

### Sc√©nario 1 : Premi√®re utilisation
```powershell
# 1. Installer Ollama
.\setup-ollama.ps1

# 2. Red√©marrer si demand√©
Restart-Computer

# 3. T√©l√©charger le mod√®le
ollama pull qwen2.5:7b

# 4. D√©marrer QUIZO
cd python_api ; python app.py
# Nouveau terminal : npm run dev

# 5. Cr√©er un quiz avec "LLM Local"
```

---

### Sc√©nario 2 : Utilisation quotidienne
```powershell
# V√©rifier qu'Ollama tourne
Get-Process ollama

# D√©marrer le backend
cd python_api ; python app.py

# D√©marrer le frontend (nouveau terminal)
npm run dev

# Aller sur http://localhost:5173
```

---

### Sc√©nario 3 : Tester un nouveau mod√®le
```powershell
# T√©l√©charger
ollama pull mistral:7b

# Tester interactivement
ollama run mistral:7b

# Demander : "G√©n√®re 3 questions sur la photosynth√®se"

# Si bon, mettre √† jour le .env
cd python_api
notepad .env
# Changer : OLLAMA_MODEL=mistral:7b

# Red√©marrer le backend
```

---

### Sc√©nario 4 : PC lent, optimiser
```powershell
# Supprimer les gros mod√®les
ollama rm qwen2.5:14b

# Installer le mod√®le l√©ger
ollama pull phi3:mini

# Fermer les applications
# Chrome, VSCode, etc.

# G√©n√©rer avec moins de questions
# Dans QUIZO : 3 questions au lieu de 10
```

---

### Sc√©nario 5 : Erreur "mod√®le non trouv√©"
```powershell
# V√©rifier les mod√®les
ollama list

# Si le mod√®le manque, le t√©l√©charger
ollama pull qwen2.5:7b

# V√©rifier le .env
cd python_api
notepad .env
# S'assurer que OLLAMA_MODEL correspond √† un mod√®le install√©

# Red√©marrer le backend
```

---

## üìö Documentation

### Guides disponibles
```powershell
# Ouvrir les guides
notepad OLLAMA_QUICKSTART.md          # D√©marrage rapide
notepad LOCAL_LLM_GUIDE.md            # Guide complet
notepad LOCAL_LLM_INTEGRATION.md      # Int√©gration technique
notepad SUMMARY_LLM_LOCAL.md          # R√©sum√© impl√©mentation
notepad STUDENT_GUIDE_LLM.md          # Guide √©tudiant
notepad COMMANDS_QUICK_REF.md         # Ce fichier
```

---

## üÜò Support

### Logs du backend
```powershell
# D√©marrer avec plus de logs
cd python_api
$env:LOG_LEVEL="DEBUG"
python app.py

# Les logs montreront :
# - Requ√™tes Ollama
# - R√©ponses g√©n√©r√©es
# - Erreurs d√©taill√©es
```

### V√©rifier les versions
```powershell
# Ollama
ollama --version

# Python
python --version

# Node.js
node --version

# npm
npm --version
```

### Nettoyer et recommencer
```powershell
# Backend
cd python_api
Remove-Item -Recurse -Force __pycache__
pip install -r requirements.txt

# Frontend
cd ..
Remove-Item -Recurse -Force node_modules
npm install

# Ollama
Stop-Process -Name "ollama" -Force
ollama serve
```

---

## üí° Astuces

### Astuce 1 : Cr√©er un alias pour d√©marrer QUIZO
```powershell
# Ajouter √† votre profil PowerShell
notepad $PROFILE

# Ajouter :
function Start-Quizo {
    Start-Process powershell -ArgumentList "-NoExit", "-Command", "cd 'C:\Users\SetupGame\Desktop\QUIZPROJECT\ESTS-QUIZ\python_api'; python app.py"
    Start-Sleep -Seconds 2
    Start-Process powershell -ArgumentList "-NoExit", "-Command", "cd 'C:\Users\SetupGame\Desktop\QUIZPROJECT\ESTS-QUIZ'; npm run dev"
}

# Utiliser :
Start-Quizo
```

---

### Astuce 2 : Automatiser le d√©marrage d'Ollama au d√©marrage
```powershell
# Cr√©er une t√¢che planifi√©e
$action = New-ScheduledTaskAction -Execute "ollama" -Argument "serve"
$trigger = New-ScheduledTaskTrigger -AtStartup
Register-ScheduledTask -TaskName "Ollama" -Action $action -Trigger $trigger

# D√©sactiver si n√©cessaire
Unregister-ScheduledTask -TaskName "Ollama" -Confirm:$false
```

---

### Astuce 3 : Monitoring du GPU pendant la g√©n√©ration
```powershell
# Terminal 1 : D√©marrer la g√©n√©ration dans QUIZO

# Terminal 2 : Surveiller le GPU en temps r√©el
nvidia-smi -l 1

# Appuyer sur Ctrl+C pour arr√™ter
```

---

## üîó Liens utiles

- **Ollama** : https://ollama.com
- **Models** : https://ollama.com/library
- **Qwen** : https://huggingface.co/Qwen
- **Mistral** : https://mistral.ai
- **Documentation QUIZO** : https://github.com/OmarElkhali/QUIZO

---

## üìù Notes

- Tous les mod√®les sont t√©l√©charg√©s dans : `C:\Users\<votre-nom>\.ollama\models`
- La config Ollama est dans : `C:\Users\<votre-nom>\.ollama`
- Les logs Flask sont dans la console o√π vous avez lanc√© `python app.py`

---

**Derni√®re mise √† jour : Novembre 2025**  
**Version : 1.0**
