# ü§ñ Int√©gration LLM Local dans QUIZO

Ce document explique comment QUIZO utilise des mod√®les LLM locaux (via Ollama) comme alternative gratuite et priv√©e aux API cloud.

## üìÅ Architecture

```
QUIZO/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ       ‚îî‚îÄ‚îÄ localLlmService.ts          # Client TypeScript pour Ollama
‚îú‚îÄ‚îÄ python_api/
‚îÇ   ‚îú‚îÄ‚îÄ app.py                          # Backend Flask (modifi√© pour Ollama)
‚îÇ   ‚îî‚îÄ‚îÄ ollama_service.py               # Service Python pour Ollama
‚îú‚îÄ‚îÄ LOCAL_LLM_GUIDE.md                  # Guide d√©taill√© d'impl√©mentation
‚îú‚îÄ‚îÄ OLLAMA_QUICKSTART.md                # Guide de d√©marrage rapide
‚îî‚îÄ‚îÄ .env                                # Configuration
```

## üîÑ Flux de donn√©es

### Option 1 : Via le backend Flask (recommand√©)

```
Frontend (React)
    ‚Üì
localLlmService.generateQuestionsWithLocalLLM()
    ‚Üì
Backend Flask (/api/generate avec modelType='local')
    ‚Üì
generate_with_ollama() dans app.py
    ‚Üì
Ollama API (localhost:11434)
    ‚Üì
Mod√®le local (Qwen/Llama/Mistral)
    ‚Üì
Questions g√©n√©r√©es ‚Üí Frontend
```

### Option 2 : Direct depuis le frontend

```
Frontend (React)
    ‚Üì
localLlmService.generateDirectlyWithOllama()
    ‚Üì
Ollama API (localhost:11434)
    ‚Üì
Mod√®le local
    ‚Üì
Questions g√©n√©r√©es ‚Üí Frontend
```

## üõ†Ô∏è Fichiers cr√©√©s/modifi√©s

### 1. **Backend Flask** (`python_api/app.py`)

**Modifications :**
- Ajout de la configuration Ollama (URL, mod√®le par d√©faut)
- Nouvelle fonction `generate_with_ollama(prompt, model)`
- Support de `modelType='local'` dans `/api/generate`
- Nouveau endpoint `/api/ollama/models` pour lister les mod√®les
- Mise √† jour du `/api/health` pour inclure le statut Ollama

**Code cl√© :**
```python
# Configuration
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
DEFAULT_OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2.5:7b")

# G√©n√©ration avec Ollama
if model_type == 'local':
    content = generate_with_ollama(prompt, local_model)
```

### 2. **Service Ollama Python** (`python_api/ollama_service.py`)

**Nouveau fichier** avec :
- Classe `OllamaService` pour g√©rer les interactions
- Fonction `generate_quiz_with_ollama()` pour g√©n√©rer des QCM
- Fonction `test_ollama()` pour tester l'installation
- Validation compl√®te des questions g√©n√©r√©es
- Gestion d'erreurs robuste

**Utilisation :**
```bash
cd python_api
python ollama_service.py  # Lance les tests
```

### 3. **Service Frontend** (`src/services/localLlmService.ts`)

**Nouveau fichier** avec :
- `isLocalLLMAvailable()` - V√©rifie si Ollama est accessible
- `listLocalModels()` - Liste les mod√®les t√©l√©charg√©s
- `generateQuestionsWithLocalLLM()` - G√©n√®re via le backend Flask
- `generateDirectlyWithOllama()` - G√©n√®re directement avec Ollama
- `getLocalLLMInfo()` - R√©cup√®re l'√©tat d'Ollama
- `RECOMMENDED_MODELS` - Liste des mod√®les recommand√©s

**Utilisation :**
```typescript
import { generateQuestionsWithLocalLLM } from '@/services/localLlmService';

const questions = await generateQuestionsWithLocalLLM(
  text,
  5,          // nombre de questions
  'medium',   // difficult√©
  'qwen2.5:7b' // mod√®le
);
```

### 4. **Composant React** (`src/components/LocalModelSelector.tsx`)

**Nouveau composant** pour :
- Afficher l'√©tat d'Ollama (disponible/non disponible)
- Lister les mod√®les t√©l√©charg√©s
- S√©lectionner un mod√®le
- Afficher les informations sur le mod√®le s√©lectionn√©
- Afficher les mod√®les recommand√©s
- Guide d'installation si Ollama n'est pas disponible

**Utilisation :**
```tsx
import { LocalModelSelector } from '@/components/LocalModelSelector';

<LocalModelSelector
  selectedModel={selectedModel}
  onModelChange={setSelectedModel}
  showRecommendations={true}
/>
```

## üîå Int√©gration dans CreateQuiz

Pour int√©grer dans la page de cr√©ation de quiz :

### 1. Modifier `CreateQuiz.tsx`

```tsx
import { useState } from 'react';
import { LocalModelSelector } from '@/components/LocalModelSelector';
import { generateQuestionsWithLocalLLM } from '@/services/localLlmService';

function CreateQuiz() {
  const [modelType, setModelType] = useState<'gemini' | 'chatgpt' | 'local'>('gemini');
  const [localModel, setLocalModel] = useState('qwen2.5:7b');

  const handleGenerate = async () => {
    if (modelType === 'local') {
      // Utiliser LLM local
      const questions = await generateQuestionsWithLocalLLM(
        extractedText,
        numQuestions,
        difficulty,
        localModel
      );
      setGeneratedQuestions(questions);
    } else {
      // Utiliser Gemini/ChatGPT comme avant
      // ...code existant
    }
  };

  return (
    <div>
      {/* S√©lecteur de type de mod√®le */}
      <select value={modelType} onChange={(e) => setModelType(e.target.value)}>
        <option value="gemini">Gemini (API)</option>
        <option value="chatgpt">ChatGPT (API)</option>
        <option value="local">LLM Local (Gratuit)</option>
      </select>

      {/* Afficher le s√©lecteur de mod√®le local si n√©cessaire */}
      {modelType === 'local' && (
        <LocalModelSelector
          selectedModel={localModel}
          onModelChange={setLocalModel}
        />
      )}

      <button onClick={handleGenerate}>G√©n√©rer le Quiz</button>
    </div>
  );
}
```

### 2. Modifier `aiService.ts` (optionnel)

Pour centraliser la logique de g√©n√©ration :

```typescript
import { generateQuestionsWithLocalLLM } from './localLlmService';

export const generateQuestions = async (
  text: string,
  numQuestions: number,
  difficulty: 'easy' | 'medium' | 'hard',
  modelType: 'gemini' | 'chatgpt' | 'local',
  config?: {
    apiKey?: string;
    localModel?: string;
  }
): Promise<Question[]> => {
  switch (modelType) {
    case 'local':
      return generateQuestionsWithLocalLLM(
        text,
        numQuestions,
        difficulty,
        config?.localModel || 'qwen2.5:7b'
      );
    
    case 'chatgpt':
      return generateQuestionsWithAI(text, numQuestions, difficulty, 'chatgpt', config?.apiKey);
    
    case 'gemini':
    default:
      return generateQuestionsWithAI(text, numQuestions, difficulty, 'gemini');
  }
};
```

## ‚öôÔ∏è Configuration

### Variables d'environnement

**Backend** (`python_api/.env`) :
```bash
# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b

# API Cloud (optionnelles)
GEMINI_API_KEY=your_key_here
CHATGPT_API_KEY=your_key_here

# Serveur
LOG_LEVEL=INFO
CORS_ORIGINS=http://localhost:5173,http://localhost:8080
```

**Frontend** (`.env`) :
```bash
VITE_BACKEND_URL=http://localhost:5001/api
VITE_LOCAL_LLM_URL=http://localhost:11434

# Firebase config...
VITE_FIREBASE_API_KEY=...
```

## üß™ Tests

### Tester le service Ollama Python

```bash
cd python_api
python ollama_service.py
```

Sortie attendue :
```
üîç Test de la connexion Ollama...
‚úÖ Ollama est disponible!
üì¶ Mod√®les disponibles:
   - qwen2.5:7b
üß™ Test de g√©n√©ration avec qwen2.5:7b...
‚úÖ 2 questions g√©n√©r√©es avec succ√®s!
```

### Tester l'endpoint Flask

```bash
# D√©marrer le backend
cd python_api
python app.py

# Dans un autre terminal
curl -X POST http://localhost:5001/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Firebase est une plateforme de d√©veloppement...",
    "numQuestions": 2,
    "difficulty": "easy",
    "modelType": "local",
    "localModel": "qwen2.5:7b"
  }'
```

### Tester depuis le frontend

```typescript
import { getLocalLLMInfo } from '@/services/localLlmService';

// V√©rifier l'√©tat
const info = await getLocalLLMInfo();
console.log(info);
// { available: true, models: ['qwen2.5:7b'], message: '1 mod√®le(s) disponible(s)' }
```

## üìä Performance

### Benchmarks (PC avec RTX 3060, 16GB RAM)

| Mod√®le | Taille | 5 questions | 10 questions | RAM | Qualit√© |
|--------|--------|------------|--------------|-----|---------|
| qwen2.5:7b | 4.7 GB | ~40s | ~80s | 6 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| qwen2.5:14b | 8.5 GB | ~75s | ~150s | 10 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| llama3.1:8b | 5 GB | ~50s | ~100s | 7 GB | ‚≠ê‚≠ê‚≠ê‚≠ê |
| mistral:7b | 4.1 GB | ~35s | ~70s | 6 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| phi3:mini | 2.3 GB | ~20s | ~40s | 3 GB | ‚≠ê‚≠ê‚≠ê |
| **Gemini API** | - | ~25s | ~50s | 0 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### Recommandations

- **PC modeste** (8GB RAM, pas de GPU) : `phi3:mini`
- **PC standard** (16GB RAM, GPU modeste) : `qwen2.5:7b` ou `mistral:7b`
- **PC puissant** (32GB RAM, bon GPU) : `qwen2.5:14b`
- **Production** : Gemini API ou serveur d√©di√© avec Ollama

## üêõ D√©pannage courant

### Erreur : "Ollama n'est pas disponible"

**Solution :**
```powershell
# V√©rifier qu'Ollama tourne
Get-Process ollama

# Si non, d√©marrer manuellement
ollama serve
```

### Erreur : "Le mod√®le X n'est pas disponible"

**Solution :**
```powershell
# V√©rifier les mod√®les
ollama list

# T√©l√©charger le mod√®le manquant
ollama pull qwen2.5:7b
```

### G√©n√©ration trop lente

**Solutions :**
1. Utiliser un mod√®le plus petit (`phi3:mini`)
2. R√©duire le texte source (<5000 caract√®res)
3. V√©rifier que le GPU est utilis√© (`nvidia-smi`)
4. Fermer les autres applications

### Erreur de m√©moire

**Solutions :**
1. Utiliser `phi3:mini` (n√©cessite seulement 4GB RAM)
2. Fermer Chrome/VSCode
3. Augmenter le swap/pagefile Windows

## üöÄ D√©ploiement

### D√©veloppement local

```bash
# Backend
cd python_api
python app.py

# Frontend
npm run dev
```

### Production

**Option 1 : Serveur d√©di√© avec Ollama**
- Installer Ollama sur le serveur
- Configurer OLLAMA_BASE_URL vers le serveur
- Utiliser un load balancer pour plusieurs instances

**Option 2 : Hybrid (Cloud + Local)**
- Utiliser Gemini/ChatGPT pour la production
- Garder Ollama pour le d√©veloppement local

## üìö Ressources

- [Guide complet](./LOCAL_LLM_GUIDE.md)
- [D√©marrage rapide](./OLLAMA_QUICKSTART.md)
- [Documentation Ollama](https://ollama.com)
- [Mod√®les Qwen](https://huggingface.co/Qwen)

## üéØ Prochaines √©tapes

1. ‚úÖ Installation et test d'Ollama
2. ‚¨ú Int√©gration dans CreateQuiz.tsx
3. ‚¨ú Tests avec diff√©rents mod√®les
4. ‚¨ú Optimisation des prompts
5. ‚¨ú Cache des r√©ponses
6. ‚¨ú Fine-tuning d'un mod√®le pour QUIZO

---

**Bon coding ! üéì**
