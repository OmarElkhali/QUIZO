# -*- coding: utf-8 -*-
"""
Service d'int√©gration Ollama pour QUIZO
Utilise des mod√®les LLM locaux (Qwen, Llama, Mistral) via Ollama
"""

import os
import json
import logging
import requests
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

# Configuration Ollama
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
DEFAULT_MODEL = os.getenv("OLLAMA_MODEL", "qwen3-vl:4b")  # Mod√®le rapide avec vision

class OllamaService:
    """Service pour interagir avec Ollama API"""
    
    def __init__(self, base_url: str = OLLAMA_BASE_URL, model: str = DEFAULT_MODEL):
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.api_url = f"{self.base_url}/api"
        logger.info(f"OllamaService initialis√© avec mod√®le: {model}, URL: {base_url}")
    
    def is_available(self) -> bool:
        """V√©rifie si Ollama est disponible"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=2)
            return response.status_code == 200
        except Exception as e:
            logger.warning(f"Ollama non disponible: {e}")
            return False
    
    def list_models(self) -> List[str]:
        """Liste les mod√®les disponibles"""
        try:
            response = requests.get(f"{self.api_url}/tags", timeout=5)
            response.raise_for_status()
            data = response.json()
            models = [model['name'] for model in data.get('models', [])]
            logger.info(f"Mod√®les Ollama disponibles: {models}")
            return models
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des mod√®les: {e}")
            return []
    
    def generate(self, prompt: str, stream: bool = False, options: Optional[Dict] = None) -> str:
        """
        G√©n√®re du texte avec Ollama
        
        Args:
            prompt: Le prompt √† envoyer au mod√®le
            stream: Si True, retourne un stream (non impl√©ment√© ici)
            options: Options suppl√©mentaires (temperature, top_p, etc.)
        
        Returns:
            Le texte g√©n√©r√©
        """
        try:
            logger.info(f"G√©n√©ration avec Ollama, mod√®le: {self.model}")
            logger.debug(f"Prompt (premiers 200 caract√®res): {prompt[:200]}...")
            
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": options or {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "top_k": 40,
                    "num_predict": 4096,  # Augmenter la limite de tokens g√©n√©r√©s
                }
            }
            
            # Timeout g√©n√©reux pour la g√©n√©ration
            response = requests.post(
                f"{self.api_url}/generate",
                json=payload,
                timeout=600  # 10 minutes max pour la premi√®re g√©n√©ration
            )
            response.raise_for_status()
            
            result = response.json()
            generated_text = result.get('response', '')
            
            logger.info(f"G√©n√©ration r√©ussie: {len(generated_text)} caract√®res")
            logger.debug(f"R√©ponse (premiers 200 caract√®res): {generated_text[:200]}...")
            
            return generated_text
            
        except requests.exceptions.Timeout:
            logger.error("Timeout lors de la g√©n√©ration Ollama")
            raise ValueError("La g√©n√©ration a pris trop de temps. Essayez avec un prompt plus court.")
        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur lors de la requ√™te Ollama: {e}")
            raise ValueError(f"Erreur lors de la communication avec Ollama: {str(e)}")
        except Exception as e:
            logger.error(f"Erreur inattendue lors de la g√©n√©ration: {e}")
            raise ValueError(f"Erreur lors de la g√©n√©ration: {str(e)}")
    
    def chat(self, messages: List[Dict[str, str]], options: Optional[Dict] = None) -> str:
        """
        Interface chat avec Ollama (compatible OpenAI format)
        
        Args:
            messages: Liste de messages [{role: "user|assistant|system", content: "..."}]
            options: Options suppl√©mentaires
        
        Returns:
            La r√©ponse du mod√®le
        """
        try:
            logger.info(f"Chat avec Ollama, mod√®le: {self.model}, {len(messages)} messages")
            
            payload = {
                "model": self.model,
                "messages": messages,
                "stream": False,
                "options": options or {
                    "temperature": 0.7,
                    "top_p": 0.9,
                }
            }
            
            response = requests.post(
                f"{self.api_url}/chat",
                json=payload,
                timeout=180
            )
            response.raise_for_status()
            
            result = response.json()
            assistant_message = result.get('message', {}).get('content', '')
            
            logger.info(f"Chat r√©ussi: {len(assistant_message)} caract√®res")
            return assistant_message
            
        except Exception as e:
            logger.error(f"Erreur lors du chat Ollama: {e}")
            raise ValueError(f"Erreur lors du chat: {str(e)}")


def generate_quiz_with_ollama(
    text: str,
    num_questions: int = 5,
    difficulty: str = "medium",
    model: str = DEFAULT_MODEL
) -> List[Dict[str, Any]]:
    """
    G√©n√®re des questions QCM en utilisant Ollama
    
    Args:
        text: Le texte source pour g√©n√©rer les questions
        num_questions: Nombre de questions √† g√©n√©rer
        difficulty: Difficult√© (easy, medium, hard)
        model: Mod√®le Ollama √† utiliser
    
    Returns:
        Liste de questions au format QUIZO
    """
    
    # Cr√©er le service Ollama
    ollama = OllamaService(model=model)
    
    # V√©rifier la disponibilit√©
    if not ollama.is_available():
        raise ValueError("Ollama n'est pas disponible. Assurez-vous qu'il est install√© et en cours d'ex√©cution.")
    
    # V√©rifier que le mod√®le est disponible
    available_models = ollama.list_models()
    if model not in available_models:
        logger.warning(f"Mod√®le {model} non trouv√©. Mod√®les disponibles: {available_models}")
        if available_models:
            model = available_models[0]
            logger.info(f"Utilisation du mod√®le alternatif: {model}")
            ollama.model = model
        else:
            raise ValueError("Aucun mod√®le Ollama disponible. T√©l√©chargez un mod√®le avec 'ollama pull qwen2.5:7b'")
    
    # Construction du prompt optimis√© pour Ollama
    prompt = f"""Tu es un expert en cr√©ation de quiz √©ducatifs. G√©n√®re {num_questions} questions QCM en fran√ßais de niveau {difficulty}.

TEXTE SOURCE :
{text[:5000]}

INSTRUCTIONS STRICTES :
1. Cr√©e exactement {num_questions} questions bas√©es UNIQUEMENT sur le texte source
2. Chaque question doit avoir EXACTEMENT 4 options
3. UNE SEULE option doit avoir "isCorrect": true
4. L'explication doit citer ou r√©f√©rencer le texte source
5. Utilise le niveau de difficult√©: {difficulty}

FORMAT JSON REQUIS (SANS markdown, SANS ```json) :
{{
  "questions": [
    {{
      "text": "Question claire et pr√©cise ?",
      "options": [
        {{"text": "R√©ponse correcte", "isCorrect": true}},
        {{"text": "R√©ponse plausible mais fausse", "isCorrect": false}},
        {{"text": "Autre r√©ponse plausible mais fausse", "isCorrect": false}},
        {{"text": "Derni√®re r√©ponse plausible mais fausse", "isCorrect": false}}
      ],
      "explanation": "Explication bas√©e sur le texte source",
      "difficulty": "{difficulty}"
    }}
  ]
}}

IMPORTANT : Retourne UNIQUEMENT le JSON, sans texte avant ou apr√®s, sans balises markdown.
"""
    
    logger.info(f"G√©n√©ration de {num_questions} questions avec Ollama ({model})")
    
    # G√©n√©rer avec Ollama
    try:
        response_text = ollama.generate(
            prompt,
            options={
                "temperature": 0.7,  # Un peu de cr√©ativit√©
                "top_p": 0.9,
                "top_k": 40,
                "num_predict": 2048,  # Limite de tokens g√©n√©r√©s
            }
        )
        
        logger.debug(f"R√©ponse brute Ollama: {response_text[:500]}...")
        
        # Nettoyer la r√©ponse (enlever markdown si pr√©sent)
        response_text = response_text.strip()
        if response_text.startswith('```json'):
            response_text = response_text[7:]  # Enlever ```json
        if response_text.startswith('```'):
            response_text = response_text[3:]
        if response_text.endswith('```'):
            response_text = response_text[:-3]
        response_text = response_text.strip()
        
        # Parser le JSON
        try:
            data = json.loads(response_text)
            questions = data.get('questions', [])
        except json.JSONDecodeError as e:
            logger.error(f"Erreur de parsing JSON: {e}")
            logger.error(f"R√©ponse probl√©matique: {response_text[:1000]}")
            
            # Tenter d'extraire le JSON avec regex
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                data = json.loads(json_str)
                questions = data.get('questions', [])
            else:
                raise ValueError("Impossible d'extraire le JSON de la r√©ponse")
        
        # Valider et formater les questions
        validated_questions = []
        for idx, q in enumerate(questions[:num_questions]):
            # Validation basique
            if not q.get('text') or not q.get('options') or not q.get('explanation'):
                logger.warning(f"Question {idx+1} invalide, ignor√©e")
                continue
            
            if len(q['options']) != 4:
                logger.warning(f"Question {idx+1} n'a pas 4 options, ignor√©e")
                continue
            
            # Compter les r√©ponses correctes
            correct_count = sum(1 for opt in q['options'] if opt.get('isCorrect', False))
            if correct_count != 1:
                logger.warning(f"Question {idx+1} n'a pas exactement 1 r√©ponse correcte ({correct_count}), correction...")
                # Forcer la premi√®re option comme correcte si aucune n'est marqu√©e
                if correct_count == 0:
                    q['options'][0]['isCorrect'] = True
                # Si plusieurs sont correctes, garder seulement la premi√®re
                elif correct_count > 1:
                    found_first = False
                    for opt in q['options']:
                        if opt.get('isCorrect', False):
                            if not found_first:
                                found_first = True
                            else:
                                opt['isCorrect'] = False
            
            # Ajouter les IDs
            q['id'] = f"q{idx+1}"
            for opt_idx, opt in enumerate(q['options']):
                if 'id' not in opt:
                    opt['id'] = f"{q['id']}_{chr(97 + opt_idx)}"  # a, b, c, d
            
            # Ajouter la difficult√© si absente
            if 'difficulty' not in q:
                q['difficulty'] = difficulty
            
            validated_questions.append(q)
            logger.debug(f"Question {idx+1} valid√©e: {q['text'][:50]}...")
        
        if not validated_questions:
            raise ValueError("Aucune question valide n'a √©t√© g√©n√©r√©e")
        
        logger.info(f"{len(validated_questions)} questions valid√©es sur {len(questions)} g√©n√©r√©es")
        return validated_questions
        
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration avec Ollama: {e}")
        raise


# Fonction helper pour tester
def test_ollama():
    """Teste la connexion et g√©n√©ration Ollama"""
    print("üîç Test de la connexion Ollama...\n")
    
    ollama = OllamaService()
    
    # Test disponibilit√©
    if not ollama.is_available():
        print("‚ùå Ollama n'est pas disponible!")
        print("   Assurez-vous qu'Ollama est install√© et en cours d'ex√©cution")
        print("   Commande: ollama serve")
        return False
    
    print("‚úÖ Ollama est disponible!\n")
    
    # Lister les mod√®les
    print("üì¶ Mod√®les disponibles:")
    models = ollama.list_models()
    for model in models:
        print(f"   - {model}")
    print()
    
    if not models:
        print("‚ö†Ô∏è  Aucun mod√®le trouv√©!")
        print("   T√©l√©chargez un mod√®le avec: ollama pull qwen2.5:7b")
        return False
    
    # Test de g√©n√©ration
    print(f"üß™ Test de g√©n√©ration avec qwen3:4b...\n")
    
    # Utiliser qwen3:4b (sans thinking)
    model_to_use = "qwen3:4b" if "qwen3:4b" in models else models[0]
    
    test_text = """
    Python est un langage de programmation populaire cr√©√© par Guido van Rossum en 1991.
    Il est connu pour sa syntaxe simple et sa grande lisibilit√©.
    """
    
    try:
        print("‚è≥ G√©n√©ration en cours (peut prendre 30-60 secondes)...\n")
        questions = generate_quiz_with_ollama(
            text=test_text,
            num_questions=1,  # 1 seule question pour tester
            difficulty="easy",
            model=model_to_use
        )
        
        print(f"‚úÖ {len(questions)} questions g√©n√©r√©es avec succ√®s!\n")
        
        for i, q in enumerate(questions, 1):
            print(f"Question {i}: {q['text']}")
            print(f"  Difficult√©: {q['difficulty']}")
            print(f"  Options: {len(q['options'])}")
            correct = [opt['text'] for opt in q['options'] if opt.get('isCorrect')]
            print(f"  R√©ponse correcte: {correct[0] if correct else 'N/A'}")
            print()
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la g√©n√©ration: {e}")
        return False


if __name__ == "__main__":
    # Lancer le test
    test_ollama()
